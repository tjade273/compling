\documentclass[doc,natbib,12pt]{apa6}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{setspace} %%Enables \doublespacing command for double linespacing
\usepackage{tipa}
\usepackage{qtree}
\usepackage{float}

\title{Identifying Borrowing in large word lists}
\shorttitle{Borrowing Detection}
\author{Tjaden Hess}
\affiliation{Cornell University}
%opening

\begin{document}

\maketitle
\doublespacing
\section{ Background }
Traditionally, historical linguists have studied genetic relationships between languages with the help of the Comparative Method, which allows extremely accurate reconstructions of both the proto-language for the language family of interest and the phylogenetic tree expressing the genetic relationships within the language family. Recently, there has been an interest in automating this process, via automatic cognate detection and phylogenetic reconstruction, and several successful algorithms and pipelines have been proposed \citep{List2017a}.

The newly emergent field of computational comparative linguistics borrows heavily from the more established domain of computational biology, adapting algorithms for gene sequence alignment and taxa clustering to the problems of phonetic sequence alignment and cognate clustering. Language, however, changes in a much more restricted manner than do nucleotide sequences; sound changes in human language are highly directional and conditioned solely by phonetic environment, as opposed to the uniformly random process that drives biological evolution. The regularity of sound change allows linguists to differentiate ``true'' cognate words--- those derived from a single proto-form through Neogramarian sound-change--- from words acquired through language contact \citep[p. 108]{Campbell1999}.

Because ``true'' cognates allow reconstruction of proto-forms, a disproportionate quantity of research has been aimed towards identifying cognates while eliminating and disregarding irregularities caused by borrowing and analogy. For my project, I chose to look instead at computational detection of borrowing and its application to areal linguistics.

\section{Methods}

The central hypothesis of my project is that loanwords satisfy two properties: high phonetic similarity and poor regularity. The goal, then, is to construct scoring schema which can quantify both surface-level phonetic similarity and underlying genetic correspondence.

\subsection{Alignment}

In order to even begin an investigation into computational comparative linguistics, we must at the very least have a method for aligning sequences of phonemes so that we can identify corresponding segments in potential cognates. For pairwise alignments, the Needleman-Wunsch algorithm can be used to optimally align a pair of sequences using any scoring metric \citep{Needleman1970}. Finding optimal alignments for larger sets of words is much more difficult; while optimal algorithms exist for multiple alignment, the problem is NP-hard and so we rely on heuristic methods to give reasonable alignments. The most common technique, and the one used in my project, is \emph{progressive alignment}, in which we align sequences sequentially along a ``guide tree'', which is constructed via a clustering algorithm from a distance matrix derived from pairwise alignments. While progressive alignment is not perfect, when combined with pre- and post-processing techniques it gives a very reasonable alignment in only $O(n^3)$ time \citep{Feng1987}. 

\subsection{Pairwise Alignment}

In order to measure the similarity between segments, I use an adapted version of a method, first introduced by \citet{Dolgopolsky1986}, which quantifies the linguistic notion of phonetic similarity used in manual word alignment by classifying consonants into classes defined by laboratory confusion testing. More recently, a study by \citet{Turchin2010} quantified the pairwise coincidence of sound classes in manually aligned cognate sets, providing a ready-to-use similarity scoring system. \citet{List2012a} later extended this scoring scheme into a model call SCA, the current best performing algorithm, but for ease of implementation I used Dolgopolsky's method, which List dubs DOLGO.

To create the pairwise alignments, I use a dynamic programming technique which operates on a matrix representing optimal subsequence alignments. Each cell of the matrix is filled inductively by the rule 
\begin{equation}
M_{i,j} = \max\begin{cases}
F_{i,j-1} + S(-, B_j)\\
F_{i-1,j} + S(A_i, -)\\
F_{i-1, j-1} + S(A_i, B_j)
\end{cases}
\end{equation}
where $A$ and $B$ are the sequences to align, $S$ is the scoring function, and $-$ represents a gap (insertion or deletion).

Once the matrix is filled, we backtrack from $M_{i,j}$ to $M_{0,0}$, which gives the optimal pairwise alignment. The value of $M_{i,j}$ is the similarity score for the alignment, which will be used in calculating distance measurements for the multiple-sequence alignment. 

The ability to define an arbitrary scoring function for pairs of sequences and gaps provides allows us to operate in several modes, depending largely on the penalty that we assign to insertions and deletions. In my implementation, I allow for this penalty to vary depending on whether the insert/delete operation is \emph{opening} a new gap or \emph{continuing} an existing gap. By manipulating this parameter, we can move along a spectrum between global and local alignment. In a global alignment, we attempt to align the full string, independent of subsequence structure while in a local alignment we care only about the best aligned subsequences and do not penalize large gaps (Fig.~\ref{figure:1}). Advanced alignment algorithms make use of a wide variety of these modes in a ``library'' approach, but for simplicity I use the harmonic mean of a global mode and a local mode as my similarity metric.


\begin{figure}[h]
	
	\begin{tabular}{|l||ccccccccccccccc||c|}
		\hline
		Seq. 1 & A&B&C&X&X&X&X&X&X&A&B&C&X&X&X&Score:\\
		Needleman & A&B&C&-&-&-&-&-&-&A&B&C&-&-&-&-3\\
		Smith & A&B&C&-&-&-&-&-&-&A&B&C&&&&30\\\hline
	\end{tabular}
	\caption{Global vs. Local Alignment}
	\label{figure:1}
\end{figure}



It is also possible to extend the alignment algorithm so as to take into account phonetic environment, which is an important factor in measuring sound change. While I did not have time to implement this in the code base, the SCA algorithm allows for encoding conditioning prosodic environments in the tokens themselves, which can dramatically improve alignment results.

\subsubsection{Evaluation}

There are several measures of performance for sequence alignment, each with its own advantages and shortcomings \citep{List2012b}; here I list the perfect alignments score (PAS), which is the percentage of alignments identical to the gold standard, the column score (CS), which reflects the percentage of correct columns in the test data, and the sum-of-pairs score (SPS), which is the proportion of all residue pairs in the gold standard that are reflected in the test data. The results of the evaluation can be seen in Table~\ref{table:1} compared against the SCA algorithm from the LingPy library, run on the same data. I computed the scores using LingPy's evaluation functions \citep{List2016}. The gold standard was a dataset of 16,609 words across 95 languages compiled by \citet{List2014} and available at \url{http://dx.doi.org/10.5281/zenodo.11877}.
\begin{table}[h]
	\centering{
	\begin{tabular}{|l||c|c|c|}
		\hline
		Model & PAS & CS & SPS \\
		\hline\hline
		SCA & 88.59 & 92.32 & 95.93\\ \hline
		DOLGO & 81.12 & 87.13 & 92.72 \\\hline
	\end{tabular}
	\caption{Pairwise Sequence Alignments}
	\label{table:1}}
\end{table}

\subsection{Multiple Alignment}

In order to align multiple sequences, I use the heuristic approach of progressive alignment, which traditionally proceeds in three steps: \begin{enumerate}
	\item A matrix is computed from pairwise distance scores are calculated for all sequences
	\item A guide tree is constructed using a clustering algorithm 
	\item Sequences are aligned sequentially along the guide tre
\end{enumerate}

Generally the implementation takes the form of a matrix-based dynamic programming algorithm, but I found it simpler to implement as a recursive function, memoized with a hash table. I also combined steps 2 and 3, merging alignments as the tree is constructed. The progressive technique demonstrates significant flexibility; at each step we merge two alignments by considering each column of the alignments to be a token, then apply the Needleman-Wunsch algorithm, parameterized by a scoring function over pairs of aligned sequences. This allows us to encode scoring functions which reward construction of columns which fall inside established sound correspondence sets. 

\subsubsection{Clustering}

In order to construct the guide tree, we need a \emph{distance} metric, while our pairwise alignment algorithm only gives pairwise similarity. The major issue with converting between the two interpretations of the measure is that the metric must be normalized in order to be directly comparable between alignments of different lengths. 
To adjust for this disparity, we use a technique from the ALINE algorithm by \citet{Downey2008}. 
\begin{equation}
d = 1- \frac{2s}{s_1+s_2}
\end{equation}
where $s$ is the raw similarity score from the pairwise alignment, and $s_1,\ s_2$ are the scores obtained from aligning the sequences with themselves. This normalizes for length and gives a distance in the range $[0,1]$. 

Given the distance metric, I used the Unweighted Pair Group Method with Arithmetic Mean (UPGMA) algorithm to construct the guide tree. UPGMA constructs a rooted tree with branch lengths from a distance matrix by iteratively joining the closest two nodes and then defining the distance to all of the other nodes as the arithmetic mean of all of the nodes in the two branches. Because UPGMA assumes ultrametricity, i.e. that all evolution happens at a constant rate across all branches, it is less accurate than newer methods like Neighbor-Joining, but its simplicity and fast running time make it a good choice for large datasets.


As a simple example, we look at the multiple alignment of the word ``swear'' across several Germanic languages: 

\begin{figure}[h!]
	\centering
	\begin{tabular}{|l|c|}\hline
		Language & \emph{swear}\\\hline
		Danish & \textipa{svEPo} \\
		Dutch & \textipa{zve\*r} \\
		English & \textipa{swE@}\\
		German & \textipa{Sv\o:K} \\	\hline	
	%	dan_std s.v.ɛ.ʔ.o
	%	dut_std z.v.e.ɹ
	%	eng_std s.w.ɛə
	%	grm_std ʃ.v.øː.ʁ
	%	ice_std s.v.ɛ.r.j.a
	%	nor_sta s.v.ɛ.ʁ.k
	%	sco_std s.w.eə.ɹ
	%	swe_sto s.v.aː.r
	%	yid_nyo ʃ.v.ɛ.ɾ
		
	\end{tabular} 
\end{figure}


First we compute the pairwise distance scores for the words:

\begin{equation*}\begin{bmatrix}
&        Danish & Dutch & English & German\\
Danish &    -    &  0.866 & 1.461 &  0.866 \\      
Dutch  &     -    &   -      &   0.833 & 0 \\
English &   -    &      -    &     -     & 0.833\\
German & - & - & - & -
\end{bmatrix}
\end{equation*}

We see that German and Dutch are the closest nodes, so we pairwise align them. We continue a second time, iterating up the tree to give the partial tree structure: 
\begin{figure}[H]
	\centering
	\hspace{1em}
	\Tree[.? [.{\texttt{\textipa{z.v.e.\*r}}\\
		\texttt{\textipa{S.v.\o:.K}}} \textipa{Sv\o:K}  \textipa{zve\*r} ] [.{\textipa{s.w.E@.-.-}\\\textipa{s.v.E.P.o}} \textipa{swE@} \textipa{svEPo} ]  ]
\end{figure}
Finally we use each column of the two multiple-alignments as an individual token in the global alignment algorithm, to produce the final multiple alignment 

\begin{figure}[H]
	\centering
	\begin{tabular}{|ccccc|}\hline
	z & v & e & \textipa{\*r} & - \\
	\textipa{S} & v & \textipa{\o:} & \textipa{K} &-\\
	s & w & \textipa{E@} & - & -\\
	s & v & \textipa{E} & \textipa{P} & o \\\hline
	\end{tabular}
\end{figure} 

\subsubsection{Evaluation}

To evaluate the performance of my multiple sequence alignment algorithm, I again used the sum-of-pairs score, and additionally the Jaccard score, which is simply the number segments in correct positions divided by the total number of segments. In multiple-alignment tests on 100 sets of phoneme sequences, my implementation slightly outperformed the SCA algorithm, although I believe that most of the discrepancy can be attributed to my handling of unknown IPA characters, which I attempted to coerce into the most similar recognized token, while the SCA implementation in LingPy simply assumes unknown IPA tokens to be vowels. 


\begin{table}[h]
	\centering{
		\begin{tabular}{|l||c|c|}
			\hline
			Model & SP & JC \\
			\hline\hline
			SCA & 90.11 & 85.03\\ \hline
			DOLGO & 92.12 & 87.79  \\\hline
		\end{tabular}
		\caption{Multiple Sequence Alignment}
		\label{table:2}}
\end{table}


\subsection{Cognate detection}

The UPGMA, besides providing a guide tree for multiple alignment also allows us to cluster words that we can hypothesize to be cognates by providing a cutoff for the maximal distance between nodes that we will join. Thus, we can feed the algorithm a list of unlabeled words and it will produce a list of possible cognate sets, along with multiple-sequence alignments for each set.  



\subsubsection{Parameters}
Determining the cutoff parameter for the clustering algorithm can be difficult, as the level of phonetic similarity within cognate classes varies dramatically by language. The cutoff can be first approximated by a human, and then incrementally decreased through the iterations of the algorithm, or it can be estimated through computational methods like gradient descent. I found that first training the cutoff parameter on a small subset of the data and then using the resulting value for the rest of the analysis gave good results, i.e. the level of divergence between cognates does not change much between semantic domains, so training on a small subset of a Swadesh list is sufficient. 


\section{Language-Specific scoring}

The final step in identifying cognates in wordlists is to create language-specific scoring schemes which reflect the underlying regularity of sound correspondences. In my implementation, I used an approach by \citet{Kessler2001} in which the scoring function $S_{P,Q}(x,y)$ for a residue pair $x,y$ in alignments between languages $P$ and $Q$ is derived from the \emph{attested} distribution of the residue pair in the data as contrasted with the \emph{expected} distribution. To calculate the expected distribution, I simply count the number of times the residue pair appears for the languages in the initial multiple-alignment analysis, which provides a measure of the frequency of the residue pair among likely cognates. The expected distribution is derived by performing pairwise alignments of randomly permutations of the input data, which represents the expected distribution of the pair across unrelated words. The  score is then calculated as a log odds ratio: 
\begin{equation}
S_{P,Q}(x,y) = \frac{1}{r_1+r_2}\left(r_1\log_2\left(\frac{a^2_{P,Q}(x,y)}{e^2_{P,Q}(x,y)}\right)+r_2D(x,y)\right)
\end{equation}
where $D$ is the DOLGO, language agnostic, scoring function and $r_1,r_2$ are constants that can be adjusted to, for instance, prioritize regularity over phonetic similarity and vice versa.

\section{Evaluation}

I tested the cognate detection component of my system on a list of 817 words from Germanic languages, and computed the precision, recall, and F-score against a gold standard, using several parameters for the UPGMA cutoff. The full computation from wordlist to F-score takes about two minutes. 

\begin{table}[h]
	\centering
	\begin{tabular}{|l|ccc|}\hline
		Cutoff Value & Precision & Recall & F-Score\\\hline
		0 & .758 & .592 & .665\\
		25 & .752 & .939 & .835\\
		$\infty$ & .452 & .939 & .610 \\\hline
	\end{tabular}
	\caption{Cognate Detection Scores}
\end{table}

\section{Alternative Methods}

While the attested/expected distribution method works quite well in practice and is very fast compared to other methods, it does not strictly adhere to the Neogrammarian Hypothesis, a central tenant in historical linguistics which states that \emph{sound change is regular}. the assertion is that \emph{every} sound change is conditioned solely by its phonetic environment and occurs every time that environment occurs. While in practice this turns out to be largely definitional---irregular changes through analogy and borrowing are excluded from the term ``sound change''---it has proved to be an extremely powerful tool in language reconstruction. Attempts have been made to more explicitly enumerate all of the correspondence classes present in a data set, but the major challenge to overcome in that area is the extraction of linguistically relevant details about phonetic environment of segments solely from the IPA encoded string. 

\citet{Steiner2011} demonstrates an interesting technique using a variant of the LZ78 data compression algorithm to quantify the redundancy contained in the data, as well as several techniques to measure semantic similarity, but there is limited data to support the method's effectiveness. 

\citet{Gilman2013} implemented a full Comparative Method Algorithm, but requires as input a full phylogenetic tree for the language family of interest which is often not available and requires significant human effort to construct. 

\section{Conclusion}




\section{Reflection}

Although the scope of my project changed slightly over its course, I am very happy with what I have implemented so far. I initially set out to detect borrowing in languages, but quickly found that the simple metric of ``irregularly phonetically similar'' is not sufficient to distinguish borrowings from random phonetic similarities. This is partially because the frequency of occurrence of borrowings is small in comparison to the frequency of random phonetic similarity, and partially because loanwords tend to undergo more dramatic phonetic distortion over time than I had accounted for. When I looked at the words that my alignment algorithms scored as phonetically similar, I got precision rates of under 20\% and so I decided to postpone investigation in that direction. Given a few more months, I would investigate loanword distortion patterns and attempt to quantify a more stringent test for borrowings that could the be easily applied on top of my current framework. 

The most significant issue that I ran into was simply the number of individual components that I needed to implement before the whole system began to come together, and the difficulties associated with maintaining a relatively large-scale project with interconnecting dependencies. Due to time constraints most of the project currently lives in one global namespace, which in a 1000+ line project can get very confusing. Given more time, my very first task would be to document and modularize the codebase. While all of the code is completely functional, it is undocumented and relies the OCaml toplevel to use; I would eventually like to package it and publish it on OPAM. 

I was pleasantly surprised by the quality and availability of gold-standard data sets in this field. This is largely through the excellent work of Johann-Mattis List, the author of the excellent LingPy library and whose work formed the foundation and inspiration of this project. I also made extensive use of the evaluation module of LingPy, in order to quickly process large datasets and evaluate performance.

I found that OCaml provided an excellent platform for implementing this project, as every component of the framework is deeply integrated with all of the others, such as pairwise alignment algorithms using a scoring function based on a multiple-alignment algorithm based on a clustering algorithm that is in turn derived from the same pairwise alignment algorithm. I cannot imagine building this project without first-class functions. 

\section{Appendix}

All relevant code can by found at 

\url{https://github.com/tjade273/compling}\\

All gold standard data sets are from 

\url{http://dx.doi.org/10.5281/zenodo.11877}.

%%%%%%%%%%%BIBILIOGRAPHY%%%%%%%%%%%%%%%
\newpage
\begin{small} %%Makes bib small text size
	\singlespacing %%Makes single spaced
	%\bibliographystyle{Analysis} %%bib style found in bst folder, in bibtex folder, in texmf folder.
	%\setlength{\bibsep}{0.5pt} %%Changes spacing between bib entries
	\bibliography{CompLing} %%bib database found in bib folder, in bibtex folder
	\thispagestyle{empty} %%Removes page numbers
\end{small} %%End makes bib small text size

\end{document}
 